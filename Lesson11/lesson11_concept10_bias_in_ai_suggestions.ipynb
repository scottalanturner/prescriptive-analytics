{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bias in AI Suggestions: Why Historical Data Can Be Dangerous\n",
        "\n",
        "This notebook demonstrates how AI suggestions can perpetuate bias from historical data and why validation is essential.\n",
        "\n",
        "Understanding this is critical because:\n",
        "- **AI learns from historical data** which may contain past discrimination\n",
        "- **AI can perpetuate bias** in its suggestions\n",
        "- **Bias leads to unfair decisions** and legal problems\n",
        "- **Human oversight is essential** to identify and prevent bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Concepts\n",
        "\n",
        "**Bias in Historical Data**:\n",
        "- Historical decisions may reflect past discrimination\n",
        "- Data shows patterns that perpetuate unfairness\n",
        "- AI learns these patterns and suggests them\n",
        "\n",
        "**AI Perpetuates Bias**:\n",
        "- AI suggests constraints based on historical patterns\n",
        "- If history was biased, suggestions will be biased\n",
        "- AI doesn't understand fairness - it just finds patterns\n",
        "\n",
        "**Human Oversight**:\n",
        "- Humans must review for fairness\n",
        "- Identify discriminatory patterns\n",
        "- Reject biased suggestions\n",
        "- Ensure equal opportunity\n",
        "\n",
        "**Critical insight**: AI is not neutral. It reflects the biases in its training data. Always review for bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scenario: Hiring Model Constraints\n",
        "\n",
        "An AI tool analyzes 10 years of hiring data and suggests constraints for a hiring optimization model. The historical data reflects past bias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Required Packages (Colab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (if needed in Colab)\n",
        "%pip install numpy matplotlib pandas -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Historical Data Contains Bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Historical hiring data (contains past bias)\n",
        "# In the past, certain groups were hired less often, even when qualified\n",
        "\n",
        "historical_data = {\n",
        "    'Group': ['Group A', 'Group B', 'Group C'],\n",
        "    'Qualified_Candidates': [1000, 1000, 1000],  # Equal qualifications\n",
        "    'Hired': [800, 600, 500],  # But different hiring rates (past bias)\n",
        "    'Hiring_Rate': [0.80, 0.60, 0.50]\n",
        "}\n",
        "\n",
        "df_historical = pd.DataFrame(historical_data)\n",
        "df_historical['Hiring_Rate'] = df_historical['Hired'] / df_historical['Qualified_Candidates']\n",
        "\n",
        "print(\"HISTORICAL HIRING DATA (10 years):\")\n",
        "print(\"=\" * 60)\n",
        "print(df_historical.to_string(index=False))\n",
        "print(\"\\n‚ö†Ô∏è  Notice: Equal qualifications, but different hiring rates\")\n",
        "print(\"   This reflects PAST BIAS in hiring decisions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: AI Suggests Constraints Based on Historical Patterns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AI analyzes historical data and suggests constraints\n",
        "# AI sees: Group A hired 80%, Group B 60%, Group C 50%\n",
        "# AI suggests: \"Prioritize Group A, limit Group C\"\n",
        "\n",
        "ai_suggestion = {\n",
        "    'constraint': 'Prioritize candidates from Group A',\n",
        "    'reasoning': 'Historical data shows Group A has 80% hiring rate vs 50% for Group C',\n",
        "    'confidence': 'High'\n",
        "}\n",
        "\n",
        "print(\"AI SUGGESTION (Based on Historical Data):\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Constraint: {ai_suggestion['constraint']}\")\n",
        "print(f\"Reasoning: {ai_suggestion['reasoning']}\")\n",
        "print(f\"AI Confidence: {ai_suggestion['confidence']}\")\n",
        "print(\"\\n‚ö†Ô∏è  PROBLEM: AI learned from biased historical data\")\n",
        "print(\"   The suggestion would PERPETUATE past discrimination!\")\n",
        "print(\"   Group C candidates are equally qualified but would be deprioritized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Human Review Identifies Bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Human review identifies the bias\n",
        "print(\"HUMAN REVIEW (Diversity & Inclusion Team):\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n‚ùå BIAS IDENTIFIED:\")\n",
        "print(\"   - Historical data reflects past discrimination\")\n",
        "print(\"   - All groups have equal qualifications\")\n",
        "print(\"   - Different hiring rates indicate unfair past practices\")\n",
        "print(\"   - AI suggestion would perpetuate this discrimination\")\n",
        "print(\"\\n‚úÖ CORRECTED APPROACH:\")\n",
        "print(\"   - Reject AI suggestion\")\n",
        "print(\"   - Use fair hiring principles:\")\n",
        "print(\"   - Evaluate all qualified candidates equally\")\n",
        "print(\"   - No prioritization based on group membership\")\n",
        "print(\"\\nüìä FAIR MODEL:\")\n",
        "print(\"   - Constraint: All qualified candidates evaluated equally\")\n",
        "print(\"   - No group-based prioritization\")\n",
        "print(\"   - Ensures equal opportunity\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Visualize the Problem\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the bias problem\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Left: Historical data (biased)\n",
        "ax1.bar(df_historical['Group'], df_historical['Hiring_Rate']*100, \n",
        "        color=['#FF6B6B', '#FFA07A', '#FFD700'], alpha=0.7, edgecolor='black')\n",
        "ax1.axhline(100, color='green', linestyle='--', linewidth=2, label='Equal Opportunity (100%)')\n",
        "ax1.set_ylabel('Hiring Rate (%)', fontsize=12)\n",
        "ax1.set_title('Historical Data\\n(Contains Bias)', fontsize=14, fontweight='bold')\n",
        "ax1.set_ylim(0, 100)\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Right: Fair approach\n",
        "fair_rates = [100, 100, 100]  # Equal opportunity\n",
        "ax2.bar(df_historical['Group'], fair_rates, \n",
        "        color=['#4ECDC4', '#4ECDC4', '#4ECDC4'], alpha=0.7, edgecolor='black')\n",
        "ax2.axhline(100, color='green', linestyle='--', linewidth=2, label='Equal Opportunity (100%)')\n",
        "ax2.set_ylabel('Hiring Rate (%)', fontsize=12)\n",
        "ax2.set_title('Fair Model\\n(Equal Opportunity)', fontsize=14, fontweight='bold')\n",
        "ax2.set_ylim(0, 100)\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüîç KEY INSIGHT:\")\n",
        "print(\"   Historical data shows unequal hiring (past bias)\")\n",
        "print(\"   AI learned this pattern and suggested perpetuating it\")\n",
        "print(\"   Human review identified the bias and corrected it\")\n",
        "print(\"   Fair model ensures equal opportunity for all qualified candidates\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Takeaways\n",
        "\n",
        "1. **AI is not neutral**: It learns from data, and if data contains bias, AI will learn and perpetuate that bias.\n",
        "\n",
        "2. **Historical data can be dangerous**: Past discrimination shows up as patterns that AI suggests continuing.\n",
        "\n",
        "3. **Human oversight is essential**: Humans must review AI suggestions for fairness and bias.\n",
        "\n",
        "4. **Bias leads to harm**: Discriminatory models cause real harm to individuals and violate laws.\n",
        "\n",
        "5. **Always review for bias**: Check AI suggestions against fair hiring/decision principles before using them.\n",
        "\n",
        "**This completes Lesson 11 notebooks!** You now understand simulation, visualization, and responsible AI use in prescriptive analytics.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
